# שלב 1: הרצת מודל שפה מקומי עם .NET ו-Ollama

הצעד המעשי הראשון שלי במסע למידת ה-AI היה להקים סביבת פיתוח מקומית. המטרה הייתה להיות מסוגל להריץ מודלי שפה גדולים (LLMs) על המחשב האישי שלי, ללא תלות בחיבור לאינטרנט או בשירותי ענן.

### למה להתחיל עם מודל מקומי?

* **עצמאות מלאה:** אין צורך בחיבור לאינטרנט.
* **פרטיות:** כל המידע נשאר על המחשב.
* **ללא עלויות:** התנסות חופשית וללא הגבלה.
* **מהירות:** תגובות מהירות ללא השהיות רשת.

### הכלים והטכנולוגיות

1.  **Ollama:** פלטפורמה להרצה וניהול של LLMs בקוד פתוח.
    * **אתר:** [https://ollama.com/](https://ollama.com/)

2.  **מודל Llama 3.2 8B:** מודל שפה מבית Meta.
    * **קישור בספריית Ollama:** [https://ollama.com/library/llama3](https://ollama.com/library/llama3)

3.  **.NET 9 ו-C#:** סביבת הפיתוח העיקרית שלי.

4.  **OllamaSharp:** ספריית C#   עם ה API של Ollama.

### תהליך ההתקנה

1.  **התקנת Ollama:** הורדה והתקנה של התוכנה מהאתר הרשמי.
2.  **הורדת המודל:** פתיחת הטרמינל והרצת הפקודה:
    ```bash
    ollama pull llama3.2:8b
    ```

### דרכי התממשקות שבדקתי

וידאתי שההתקנה הצליחה על ידי התחברות למודל בשלוש דרכים:

1.  **דרך ה-Console:**
    ```bash
    ollama run llama3.2:8b
    ```
    פקודה זו פותחת צ'אט ישירות בחלון הטרמינל.

2.  **אפליקציית הדסקטופ של Ollama:** ממשק משתמש גרפי ונוח לשימוש.

3.  **דרך קוד C# עם `OllamaSharp`:**
    זהו החלק החשוב ביותר עבור הפרויקט. על ידי שימוש בספרייה, אני יכול לשלב את המודל באפליקציות .NET.

    ```csharp
    // דוגמה לקוד (יש להתקין את חבילת ה-NuGet של OllamaSharp)
    using OllamaSharp;

    var ollama = new OllamaApiClient(new Uri("http://localhost:11434"));
    var result = await ollama.GetCompletion("llama3.2:8b", "Why is the sky blue?", null);
    Console.WriteLine(result.Response);
    ```

